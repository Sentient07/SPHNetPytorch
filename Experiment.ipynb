{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f95c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import os.path as osp\n",
    "\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import PointNetWithSPH\n",
    "from modelnet_dataset import ModelNet40Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954adc1",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60291f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "out_channels = (64, 256, 1024)\n",
    "kernel_radius = 1.0*np.array([0.4*0.25, 0.4*0.5, 0.4*1., 1.])\n",
    "strides = [0]*3\n",
    "nr = 2\n",
    "l_max = 3\n",
    "normalize_patches = False\n",
    "patch_size = 64\n",
    "pool_ratio = 4\n",
    "nlatent = 1024\n",
    "\n",
    "# Data params\n",
    "data_dir = '/mnt/disk1/datasets/modelnet40_ply_hdf5_2048'\n",
    "train_file = 'train_files.txt'\n",
    "test_file = 'test_files.txt'\n",
    "\n",
    "# Training params\n",
    "n_epoch = 200\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18c1f8",
   "metadata": {},
   "source": [
    "## Define and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a7a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_train0.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_train1.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_train2.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_train3.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_train4.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_test0.h5\n",
      "/mnt/disk1/datasets/modelnet40_ply_hdf5_2048/ply_data_test1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-669fd9b4a016>:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for n_e in tqdm(range(n_epoch)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819dec89626d42dcb940c8d839d58893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.358103\n",
      "Test Accuracy : 0.235390\n",
      "Train Accuracy : 0.478013\n",
      "Test Accuracy : 0.228084\n",
      "Train Accuracy : 0.514454\n",
      "Test Accuracy : 0.234984\n",
      "Train Accuracy : 0.537052\n",
      "Test Accuracy : 0.243101\n",
      "Train Accuracy : 0.558632\n",
      "Test Accuracy : 0.250812\n",
      "Train Accuracy : 0.564434\n",
      "Test Accuracy : 0.270292\n",
      "Train Accuracy : 0.581433\n",
      "Test Accuracy : 0.296266\n",
      "Train Accuracy : 0.587439\n",
      "Test Accuracy : 0.260146\n",
      "Train Accuracy : 0.604133\n",
      "Test Accuracy : 0.285714\n",
      "Train Accuracy : 0.606474\n",
      "Test Accuracy : 0.282873\n",
      "Train Accuracy : 0.617569\n",
      "Test Accuracy : 0.278815\n",
      "Train Accuracy : 0.625611\n",
      "Test Accuracy : 0.280032\n",
      "Train Accuracy : 0.628461\n",
      "Test Accuracy : 0.257305\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = PointNetWithSPH(out_channels, kernel_radius, strides, l_max, nr, patch_size,\n",
    "                        pool_ratio, nlatent)\n",
    "model = model.cuda().double()\n",
    "# Dataset and loader\n",
    "train_dataset = ModelNet40Generator('train', data_dir, train_file)\n",
    "test_dataset = ModelNet40Generator('test', data_dir, test_file)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=16, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=16, drop_last=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "cat_loss = nn.NLLLoss()\n",
    "\n",
    "train_loss_epoch, test_loss_epoch, train_acc_epoch, test_acc_epoch = [], [], [], []\n",
    "\n",
    "for n_e in tqdm(range(n_epoch)):\n",
    "    model.train()\n",
    "    train_loss_iter, test_loss_iter, test_acc_iter, train_acc_iter = [], [], [], []\n",
    "    for X, Y in train_loader:\n",
    "        X = X.cuda().double()\n",
    "        Y = Y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X.double())\n",
    "        cur_loss = cat_loss(prediction, torch.argmax(Y, axis=1))\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        cur_loss += 1e-3*l2_norm\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_iter.append(cur_loss.item())\n",
    "        train_acc = torch.sum(torch.exp(prediction).argmax(axis=1) == Y.argmax(axis=1))/int(prediction.shape[0])\n",
    "        train_acc_iter.append(train_acc.item())\n",
    "        \n",
    "    for X, Y in test_loader:\n",
    "        X = X.cuda().double()\n",
    "        Y = Y.cuda()\n",
    "        model.eval()\n",
    "        prediction = model(X.double())\n",
    "        cur_loss = cat_loss(prediction, torch.argmax(Y, axis=1))\n",
    "        test_loss_iter.append(cur_loss.item())\n",
    "        test_acc = torch.sum(torch.exp(prediction).argmax(axis=1) == Y.argmax(axis=1))/int(prediction.shape[0])\n",
    "        test_acc_iter.append(test_acc.item())\n",
    "        \n",
    "    train_loss_epoch.append(np.mean(train_loss_iter))\n",
    "    test_loss_epoch.append(np.mean(test_loss_iter))\n",
    "\n",
    "    train_acc_epoch.append(np.mean(train_acc_iter ))\n",
    "    test_acc_epoch.append(np.mean(test_acc_iter))\n",
    "    \n",
    "    \n",
    "#     print(\"Train loss : %f\" %train_loss_epoch[-1])\n",
    "    print(\"Train Accuracy : %f\" %train_acc_epoch[-1])\n",
    "#     print(\"Test loss : %f\" %test_loss_epoch[-1])\n",
    "    print(\"Test Accuracy : %f\" %test_acc_epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"ModelNet O/A\")\n",
    "# plt.plot(np.arange(len(test_acc_epoch)), test_acc_epoch, label='Test acc')\n",
    "# plt.plot(np.arange(len(train_acc_epoch)), train_acc_epoch, label='Train acc')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763f44d",
   "metadata": {},
   "source": [
    "### Unit testing for comparing patches bw TF and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07a0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def costly_cdist_torch(points, roots):\n",
    "    # This exists because torch.cdist is inconsistent\n",
    "    r0 = torch.multiply(roots, roots)\n",
    "    r0 = torch.sum(r0, dim=2, keepdim=True)\n",
    "    r1 = torch.multiply(points, points)\n",
    "    r1 = torch.sum(r1, dim=2, keepdim=True)\n",
    "    r1 = r1.transpose(2, 1)\n",
    "\n",
    "    sq_distance_mat = r0 - 2.*torch.matmul(roots, points.transpose(2, 1)) + r1\n",
    "    return sq_distance_mat\n",
    "    \n",
    "    \n",
    "def build_patch_tf(x):\n",
    "\n",
    "    assert isinstance(x, list)\n",
    "    points_pl = x[0]\n",
    "    roots = x[1]\n",
    "\n",
    "    batch_size = points_pl.get_shape()[0]\n",
    "    num_of_points = points_pl.get_shape()[1]\n",
    "    num_of_roots = roots.get_shape()[1]\n",
    "    assert(num_of_points >= 64)\n",
    "    num_samples = 64\n",
    "\n",
    "    # compute distance mat\n",
    "\n",
    "    r0 = tf.multiply(roots, roots)\n",
    "    r0 = tf.reduce_sum(r0, axis=2, keepdims=True)\n",
    "\n",
    "    r1 = tf.multiply(points_pl, points_pl)\n",
    "    r1 = tf.reduce_sum(r1, axis=2, keepdims=True)\n",
    "    r1 = tf.transpose(r1, [0, 2, 1])\n",
    "\n",
    "    sq_distance_mat = r0 - 2.*tf.matmul(roots, tf.transpose(points_pl, [0, 2, 1])) + r1\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    r = tf.multiply(points_pl, points_pl)\n",
    "    r = tf.reduce_sum(r, 2)\n",
    "    r = tf.expand_dims(r, dim=2)\n",
    "    sq_distance_mat = r - 2. * tf.matmul(points_pl, tf.transpose(points_pl, [0, 2, 1])) + tf.transpose(\n",
    "        r, [0, 2, 1])\n",
    "    \"\"\"\n",
    "    # compute patches\n",
    "    assert(num_samples <= num_of_points)\n",
    "\n",
    "    sq_patches_dist, patches_idx = tf.nn.top_k(-sq_distance_mat, k=num_samples)\n",
    "    sq_patches_dist = -sq_patches_dist\n",
    "\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    batch_idx = tf.tile(batch_idx, (1, num_of_roots, num_samples))\n",
    "    b_patches_idx = tf.stack([batch_idx, patches_idx], -1)\n",
    "\n",
    "    patches = tf.gather_nd(points_pl, b_patches_idx)\n",
    "\n",
    "\n",
    "    patches = tf.subtract(patches, tf.expand_dims(roots, axis=2))\n",
    "\n",
    "\n",
    "    return [patches, patches_idx, sq_patches_dist, sq_distance_mat]\n",
    "\n",
    "def compute_patches_torch(points, roots):\n",
    "    \"\"\"\n",
    "    Compute point-wise distance matrix, apply KDTree to get 64 neighbour for each\n",
    "    \"\"\"\n",
    "    num_of_points = points.shape[1]\n",
    "    assert(num_of_points >= 64)\n",
    "\n",
    "#     dist_mat = torch.cdist(points, roots)\n",
    "    dist_mat = costly_cdist_torch(points, roots)\n",
    "    sq_patches_dist, patches_idx = torch.topk(-dist_mat, 64*1, dim=-1, )\n",
    "    sq_patches_dist = -sq_patches_dist\n",
    "    patches = points[torch.arange(points.shape[0]).unsqueeze(1).unsqueeze(2), patches_idx]\n",
    "\n",
    "    patches = torch.subtract(patches, roots.unsqueeze(2))\n",
    "    return patches, patches_idx, sq_patches_dist, dist_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc91d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = np.random.uniform(-1, 1., (10, 1000, 3))\n",
    "points_torch = torch.from_numpy(points).double()\n",
    "points_tf = tf.constant(points, dtype=tf.float64)\n",
    "\n",
    "patches_pyt, patches_idx_pyt, sq_patches_dist_pyt, sq_distance_mat_pyt = compute_patches_torch(points_torch, points_torch)\n",
    "patches_tf, patches_idx_tf, sq_patches_dist_tf, sq_distance_mat_tf = build_patch_tf([points_tf, points_tf])\n",
    "\n",
    "np.allclose(sq_patches_dist_pyt.numpy(), sq_patches_dist_tf.numpy(), atol=1e-6)\n",
    "# np.nonzero(patches_tf.numpy()-patches_pyt.numpy())[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39190e",
   "metadata": {},
   "source": [
    "### Unit testing for comparing kernel bw pytorch and TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021d1a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000, 64, 16, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tf_spherical_harmonics import normalized_sh\n",
    "from layers import SPHConvNet\n",
    "\n",
    "\n",
    "def tf_gaussian(x, sigma):\n",
    "\n",
    "    # sigma = 3*sigma\n",
    "    x2 = tf.multiply(x, x)\n",
    "    return tf.exp(-x2 / (2. * (sigma ** 2)))\n",
    "\n",
    "def tf_sh_kernel_(X, sq_dist, nr, l_max, rad, normalize_patch=False,\n",
    "                  radial_first=True,\n",
    "                  dtype=tf.float64):\n",
    "    # Y = unnormalized_sh(X, l_max, dtype=dtype)\n",
    "    Y = normalized_sh(X, l_max, dtype=dtype, eps=0.0001)\n",
    "    sh = Y\n",
    "\n",
    "    dist = tf.sqrt(tf.maximum(sq_dist, 0.0001))\n",
    "\n",
    "    dist = tf.expand_dims(dist, axis=-1)\n",
    "    r = tf.cast(tf.reshape(tf.linspace(start=0., stop=rad, num=nr), shape=(1, 1, 1, nr)), dtype)\n",
    "    r = tf.subtract(dist, r)\n",
    "    sigma = (rad/(nr - 1))\n",
    "    radial_weights = tf_gaussian(r, sigma)\n",
    "    if radial_first:\n",
    "        Y = tf.expand_dims(Y, axis=-2)\n",
    "        radial_weights = tf.expand_dims(radial_weights, axis=-1)\n",
    "    else:\n",
    "        Y = tf.expand_dims(Y, axis=-1)\n",
    "        radial_weights = tf.expand_dims(radial_weights, axis=-2)\n",
    "\n",
    "    y = tf.multiply(Y, radial_weights)\n",
    "\n",
    "    # y = tf.expand_dims(Y, axis=-1)\n",
    "    # y = tf.tile(y, multiples=(1, 1, 1, 1, 3))\n",
    "\n",
    "\n",
    "    y_w = tf.expand_dims(tf.expand_dims(y[:, :, :, 0, 0], axis=-1), axis=-1)\n",
    "    y_w = tf.reduce_sum(y_w, axis=2, keepdims=True)\n",
    "    y = tf.divide(y, y_w + 0.000001)\n",
    "    return y, dist\n",
    "\n",
    "\n",
    "\n",
    "conv1 = SPHConvNet(3, 64,l_max, nr, patch_size, kernel_radius[0], \n",
    "                   strides=strides[0], tree_spacing=0, keep_num_points=True,\n",
    "                   max_pool = 0, l2_regularizer=1.0e-3, with_relu=True,  \n",
    "                   normalize_patch=False)\n",
    "\n",
    "tf_kernel = tf_sh_kernel_(patches_tf, sq_patches_dist_tf, nr, l_max, kernel_radius[0], normalize_patch=False,\n",
    "                          radial_first=False, dtype=tf.float64)\n",
    "torch_kernel = conv1.compute_conv_kernel(patches_pyt, sq_patches_dist_pyt)\n",
    "\n",
    "np.allclose(tf_kernel[0].numpy(), torch_kernel.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f938e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
